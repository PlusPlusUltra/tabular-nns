{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a480d1f5-18ea-4485-9110-895335b3e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisp/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [09:26:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "[LOGGED] cmc, xgboost, 0.505085, ZScore\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse, os, numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression  # for LogReg\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import typing as ty\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from argparse import Namespace\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    return json.loads(Path(path).read_text())\n",
    "\n",
    "\n",
    "DATA_PATH= \"\"\n",
    "ArrayDict = ty.Dict[str, np.ndarray]\n",
    "\n",
    "def dataname_to_numpy(dataset_name, dataset_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the dataset from the numpy files.\n",
    "\n",
    "    :param dataset_name: str\n",
    "    :param dataset_path: str\n",
    "    :return: Tuple[ArrayDict, ArrayDict, ArrayDict, Dict[str, Any]]\n",
    "    \"\"\"\n",
    "    dir_ = Path(os.path.join(DATA_PATH, dataset_path, dataset_name))\n",
    "\n",
    "    def load(item) -> ArrayDict:\n",
    "        return {\n",
    "            x: ty.cast(np.ndarray, np.load(dir_ / f'{item}_{x}.npy', allow_pickle = True))  \n",
    "            for x in ['train', 'val', 'test']\n",
    "        }\n",
    "\n",
    "    return (\n",
    "        load('N') if dir_.joinpath('N_train.npy').exists() else None,\n",
    "        load('C') if dir_.joinpath('C_train.npy').exists() else None,\n",
    "        load('y'),\n",
    "        load_json(dir_ / 'info.json'),\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_and_log_outliers(split_name, data, dataset_name, method, log_dir=\"outlier_logs\", sample_dir=\"outlier_samples\"):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    numeric_data = numeric_df.to_numpy()\n",
    "\n",
    "\n",
    "    if method == \"IsolationForest\":\n",
    "        clf = IsolationForest(random_state=42)\n",
    "        mask = clf.fit_predict(numeric_data) == 1\n",
    "    elif method == \"LocalOutlierFactor\":\n",
    "        clf = LocalOutlierFactor(n_neighbors=20)\n",
    "        mask = clf.fit_predict(numeric_data) == 1\n",
    "    elif method == \"OneClassSVM\":\n",
    "        clf = OneClassSVM(nu=0.05, kernel=\"rbf\", gamma=\"scale\")\n",
    "        mask = clf.fit_predict(numeric_data) == 1\n",
    "    elif method == \"ZScore\":\n",
    "        z = np.abs((numeric_data - numeric_data.mean(axis=0)) / (numeric_data.std(axis=0) + 1e-9))\n",
    "        mask = (z < 3).all(axis=1)\n",
    "    elif method == \"ModifiedZScore\":\n",
    "        med = np.median(numeric_data, axis=0)\n",
    "        mad = np.median(np.abs(numeric_data - med), axis=0)\n",
    "        mz = 0.6745 * (numeric_data - med) / (mad + 1e-9)\n",
    "        mask = (np.abs(mz) < 3.5).all(axis=1)\n",
    "    elif method == \"IQR\":\n",
    "        Q1 = np.percentile(numeric_data, 25, axis=0)\n",
    "        Q3 = np.percentile(numeric_data, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "        mask = ((numeric_data >= Q1 - 1.5 * IQR) & (numeric_data <= Q3 + 1.5 * IQR)).all(axis=1)\n",
    "    elif method == \"HBOS\":\n",
    "        clf = HBOS()\n",
    "        clf.fit(numeric_data)\n",
    "        mask = clf.labels_ == 0  # 0 = inlier, 1 = outlier\n",
    "\n",
    "    elif method == \"KDE\":\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=1.0)\n",
    "        kde.fit(numeric_data)\n",
    "        log_density = kde.score_samples(numeric_data)\n",
    "        threshold = np.percentile(log_density, 5)  # bottom 5% as outliers\n",
    "        mask = log_density > threshold\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    num_outliers = len(data) - mask.sum()\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    with open(os.path.join(log_dir, f\"{dataset_name}__{split_name}__{method}.txt\"), \"w\") as f:\n",
    "        f.write(f\"Outliers: {num_outliers} / {len(data)}\\n\")\n",
    "\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def make_model(model_name: str, task_type: str):\n",
    "\n",
    "    model_name = model_name.lower()\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    if model_name == \"mlp\":\n",
    "        if task_type == \"classification\":\n",
    "            return MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "        elif task_type == \"regression\":\n",
    "            return MLPRegressor(hidden_layer_sizes=(100,), max_iter=500)\n",
    "\n",
    "    elif model_name == \"xgboost\":\n",
    "        if task_type == \"classification\":\n",
    "            return XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        elif task_type == \"regression\":\n",
    "            return XGBRegressor()\n",
    "\n",
    "    elif model_name == \"linear\":\n",
    "        if task_type == \"classification\":\n",
    "            return LogisticRegression(max_iter=1000)\n",
    "        elif task_type == \"regression\":\n",
    "            return LinearRegression()\n",
    "\n",
    "    elif model_name == \"randomforest\":\n",
    "        if task_type == \"classification\":\n",
    "            return RandomForestClassifier(n_estimators=100)\n",
    "        elif task_type == \"regression\":\n",
    "            return RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "def already_logged(dataset_name, model_name, outlier_method=None, log_dir=\"results_logs\"):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, \"results.outliers.txt\" if outlier_method else \"results.txt\")\n",
    "    if not os.path.exists(log_file):\n",
    "        return False\n",
    "    with open(log_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            parts = line.split(\",\")\n",
    "            if outlier_method:\n",
    "                if len(parts) == 4 and parts[0] == dataset_name and parts[1] == model_name and parts[3] == outlier_method:\n",
    "                    return True\n",
    "            else:\n",
    "                if len(parts) == 3 and parts[0] == dataset_name and parts[1] == model_name:\n",
    "                    return True\n",
    "    return False\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simple_preprocess(X):\n",
    "    \"\"\"\n",
    "    Preprocesses only numerical columns:\n",
    "      - NaNs replaced with column mean\n",
    "      - Standardization (mean 0, std 1)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        mean_val = X[col].mean()\n",
    "        X[col] = X[col].fillna(mean_val)\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        col_mean = X[col].mean()\n",
    "        col_std = X[col].std()\n",
    "        if col_std != 0:\n",
    "            X[col] = (X[col] - col_mean) / col_std\n",
    "        else:\n",
    "            X[col] = 0  \n",
    "\n",
    "    return X.values\n",
    "\n",
    "\n",
    "def evaluate_and_log(model, X_test, y_test, dataset_name, model_name, task_type, use_outliers, outlier_method, log_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Evaluates a model and logs results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained model\n",
    "    X_test, y_test : test data\n",
    "    dataset_name : str\n",
    "    model_name : str\n",
    "    task_type : str (\"classification\" or \"regression\")\n",
    "    use_outliers : bool (True if outliers were removed before training)\n",
    "    outlier_method : str or None\n",
    "    log_dir : str (folder for results files)\n",
    "    \"\"\"\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        predictions = model.predict(X_test)\n",
    "        metric_value = accuracy_score(y_test, predictions)\n",
    "    elif task_type == \"regression\":\n",
    "        predictions = model.predict(X_test)\n",
    "        metric_value = mean_squared_error(y_test, predictions)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "    filename = \"results.outliers.txt\" if use_outliers else \"results.txt\"\n",
    "    filepath = os.path.join(log_dir, filename)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            if use_outliers:\n",
    "                search_str = f\"{dataset_name},{model_name},{outlier_method}\"\n",
    "            else:\n",
    "                search_str = f\"{dataset_name},{model_name}\"\n",
    "            if any(search_str in line for line in f):\n",
    "                print(f\"[SKIP] Experiment already exists in {filename}: {search_str}\")\n",
    "                return metric_value\n",
    "\n",
    "    with open(filepath, \"a\") as f:\n",
    "        if use_outliers:\n",
    "            f.write(f\"{dataset_name},{model_name},{metric_value:.6f},{outlier_method}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{dataset_name},{model_name},{metric_value:.6f}\\n\")\n",
    "\n",
    "    print(f\"[LOGGED] {dataset_name}, {model_name}, {metric_value:.6f}\" + (f\", {outlier_method}\" if use_outliers else \"\"))\n",
    "    return metric_value\n",
    "\n",
    "   \n",
    "def run_experiment(dataset = \"cmc\", dataset_path=\"data\", model_type = \"xgboost\", remove_outliers = \"\", outlier_method = \"ZScore\"):\n",
    "   \n",
    "    \n",
    "    if remove_outliers:\n",
    "        remove_outliers = \"--remove_outliers\"\n",
    "    args = Namespace(\n",
    "        dataset=dataset,\n",
    "        dataset_path=dataset_path,\n",
    "        model_type=model_type,\n",
    "        remove_outliers=remove_outliers,\n",
    "        outlier_method=outlier_method\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    N, C, y, info = dataname_to_numpy(args.dataset, args.dataset_path)\n",
    "    def split_dict(d): return ({k: d[k] for k in (\"train\", \"val\") if k in d}, { \"test\": d[\"test\"] }) if d else (None, None)\n",
    "    N_trval, N_test = split_dict(N)\n",
    "    C_trval, C_test = split_dict(C)\n",
    "    y_trval, y_test = split_dict(y)\n",
    "\n",
    "    if args.remove_outliers:\n",
    "        def comb(N_arr, C_arr):\n",
    "            return np.hstack([N_arr, C_arr]) if C_arr is not None else N_arr\n",
    "        for split, N_arr, C_arr in [(\"train\", N_trval[\"train\"], C_trval[\"train\"] if C_trval else None),\n",
    "                                   (\"val\", N_trval[\"val\"], C_trval[\"val\"] if C_trval else None),\n",
    "                                   (\"test\", N_test[\"test\"], C_test[\"test\"] if C_test else None)]:\n",
    "            data = comb(N_arr, C_arr)\n",
    "            mask = detect_and_log_outliers(split, data, args.dataset, args.outlier_method)\n",
    "            if split == \"train\":\n",
    "                N_trval[\"train\"] = N_arr[mask]\n",
    "                if C_trval: C_trval[\"train\"] = C_arr[mask]\n",
    "                y_trval[\"train\"] = y_trval[\"train\"][mask]\n",
    "\n",
    "    X_train = np.hstack((N_trval[\"train\"],)) \n",
    "    y_train = y_trval[\"train\"]\n",
    "    X_test = np.hstack((N_test[\"test\"],))\n",
    "    y_t = y_test[\"test\"]\n",
    "\n",
    "    X_train = simple_preprocess(X_train)\n",
    "    X_test = simple_preprocess(X_test)\n",
    "\n",
    "    task_type=info['task_type']\n",
    "    if task_type!=\"regression\":\n",
    "        task_type = \"classification\"\n",
    "    model = make_model(model_type, task_type)\n",
    "    if task_type==\"classification\":\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_t = le.fit_transform(y_t)\n",
    "\n",
    "    if model is None:\n",
    "        raise ValueError(f\"Model {args.model_type} not in models\")\n",
    "\n",
    "    if remove_outliers:\n",
    "        mask_train = detect_and_log_outliers(\"train\", X_train, dataset, outlier_method)\n",
    "        X_train = X_train[mask_train]\n",
    "        y_train = y_train[mask_train]\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(len(X_test))\n",
    "\n",
    "\n",
    "    evaluate_and_log(model, X_test, y_t, dataset, model_type, task_type, remove_outliers, outlier_method, log_dir=\".\")\n",
    "if __name__ == \"__main__\":\n",
    "    #if not already_logged()\n",
    "    run_experiment(remove_outliers = \"yes\", outlier_method = \"ZScore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e240495c-dd37-4e0b-8485-40bd659ca5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.0.3-py3-none-manylinux_2_28_x86_64.whl (253.8 MB)\n",
      "Collecting nvidia-nccl-cu12\n",
      "  Using cached nvidia_nccl_cu12-2.27.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.5 MB)\n",
      "Requirement already satisfied: numpy in /home/lisp/.local/lib/python3.10/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in /home/lisp/.local/lib/python3.10/site-packages (from xgboost) (1.15.3)\n",
      "Installing collected packages: nvidia-nccl-cu12, xgboost\n",
      "Successfully installed nvidia-nccl-cu12-2.27.7 xgboost-3.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6c0ca63-75c2-452c-8191-8f616cc14210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyod\n",
      "  Using cached pyod-2.0.5-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: scipy>=1.5.1 in /home/lisp/.local/lib/python3.10/site-packages (from pyod) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/lisp/.local/lib/python3.10/site-packages (from pyod) (2.2.6)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /home/lisp/.local/lib/python3.10/site-packages (from pyod) (1.7.0)\n",
      "Collecting numba>=0.51\n",
      "  Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Requirement already satisfied: joblib in /home/lisp/.local/lib/python3.10/site-packages (from pyod) (1.5.1)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0\n",
      "  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/lisp/.local/lib/python3.10/site-packages (from scikit-learn>=0.22.0->pyod) (3.6.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/lisp/.local/lib/python3.10/site-packages (from matplotlib->pyod) (11.3.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->pyod) (2.4.7)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lisp/.local/lib/python3.10/site-packages (from matplotlib->pyod) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lisp/.local/lib/python3.10/site-packages (from matplotlib->pyod) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.16.0)\n",
      "Installing collected packages: llvmlite, kiwisolver, fonttools, cycler, contourpy, numba, matplotlib, pyod\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 llvmlite-0.44.0 matplotlib-3.10.5 numba-0.61.2 pyod-2.0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c75041-f58a-42a5-8836-3adae3ed98b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TalentEnv",
   "language": "python",
   "name": "talentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
